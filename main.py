# -*- coding: utf-8 -*-
"""Tubes KA - Bendesrus.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oUJBdEo9CHupY6m5imo4J_UnLVlEa-5x

# Klasifikasi Aksara Kuzushiji Menggunakan Histogram of Oriented Gradients dan K-Nearest Neighbors dengan Optimasi Genetic Algorithm

# Install Deeplake
"""

! pip install "deeplake<4"

"""# Load Dataset"""

import deeplake

ds = deeplake.load("hub://activeloop/kmnist-test")

# ds = deeplake.load("hub://activeloop/kuzushiji-kanji", read_only=True)
# ds_test = deeplake.load("hub://activeloop/kmnist-test")

len(ds)

import numpy as np

all_labels = ds['labels'].numpy()
unique_labels = np.unique(all_labels)
unique_labels

"""# Eksplorasi

## Sample per kelas
"""

import matplotlib.pyplot as plt

def plot_random_samples(ds, seed=42):
    # set random seed
    np.random.seed(seed)

    # ambil semua label
    all_labels = ds['labels'].numpy()
    unique_labels = np.unique(all_labels)

    # bikin figure 7x7
    fig, axes = plt.subplots(7, 7, figsize=(10, 10))

    for i, label in enumerate(unique_labels):
        # cari semua index dengan label ini
        idxs = np.where(all_labels == label)[0]

        # pilih satu index random dari kelas ini
        idx = np.random.choice(idxs)

        # ambil image
        img = ds['images'][int(idx)].numpy()
        label_int = int(label)

        # tentukan posisi grid
        row, col = divmod(i, 7)
        axes[row, col].imshow(img.squeeze(), cmap="gray")
        axes[row, col].set_title(f"Label: {label_int}")
        axes[row, col].axis("off")

    plt.tight_layout()
    plt.show()

# contoh pemanggilan
plot_random_samples(ds, seed=14134)

"""## Distribusi kelas"""

import matplotlib.pyplot as plt
from collections import Counter

y = ds['labels'].numpy().ravel()

counter = Counter(y)

for label, count in sorted(counter.items()):
  print(f"Label {label}: {count} samples")

labels = list(counter.keys())
counts = list(counter.values())

plt.figure(figsize=(12, 5))
plt.bar(labels, counts)
plt.xlabel("Label")
plt.ylabel("Jumlah sampel")
plt.title("Distribusi Label")
plt.show()

ds['images'].shape

"""# Kenapa kNN?

Kuzushiji-MNIST merupakan dataset karakter tulisan tangan Jepang klasik dengan variabilitas intra-kelas yang tinggi, meliputi perbedaan gaya penulisan, ketebalan goresan, dan deformasi lokal. Dalam konteks ini, k-NN menunjukkan performa yang unggul karena kemampuannya memanfaatkan kemiripan lokal antar sampel tanpa mengasumsikan bentuk batas keputusan global. Setiap karakter kanji direpresentasikan sebagai vektor fitur yang mempertahankan struktur visual lokal, sehingga kedekatan antar sampel dalam ruang fitur menjadi indikator kelas yang kuat.

Selain itu, banyak karakter Kuzushiji memiliki kemiripan morfologis antar kelas (misalnya perbedaan kecil pada orientasi atau lengkungan goresan), yang menyulitkan model parametrik seperti Logistic Regression untuk membentuk pemisahan linear yang efektif. k-NN, sebagai metode non-parametrik, mampu mempertahankan detail perbedaan halus tersebut dengan membandingkan sampel secara langsung. Ketika dikombinasikan dengan fitur HOG yang menekankan orientasi gradien dan struktur tepi, k-NN menjadi sangat sesuai untuk menangkap pola tulisan tangan kanji yang kompleks dan bersifat lokal.

In simple terms, kami memilih kNN karena kNN mampu memanfaatkan kemiripan lokal antar karakter tulisan tangan yang memiliki variasi gaya tinggi dan perbedaan antar kelas yang sering kali bersifat halus. Kombinasi k-NN dengan fitur HOG memungkinkan pemodelan struktur goresan kanji secara langsung tanpa asumsi distribusi data tertentu.

- (1) Logistic Regression menunjukkan keterbatasan karena asumsi batas keputusan linear yang kurang mampu merepresentasikan variasi non-linear dan perbedaan morfologis halus antar karakter Kuzushiji-MNIST yang memiliki 49 kelas.
- (2) Decision Tree cenderung mengalami overfitting pada kelas mayoritas dan kesulitan melakukan generalisasi pada ruang fitur berdimensi tinggi dengan variasi intra-kelas yang besar.
- (3) Naive Bayes memberikan performa terendah akibat asumsi independensi fitur dan distribusi Gaussian yang tidak sesuai dengan karakteristik citra tulisan tangan yang memiliki korelasi spasial kuat. Selain itu, ketidakseimbangan kelas—di mana beberapa kelas hanya memiliki puluhan sampel—semakin memperburuk kinerja model parametrik dan berbasis asumsi.
- (4) Sebaliknya, k-NN mampu memanfaatkan kemiripan lokal antar sampel secara langsung, sehingga lebih adaptif terhadap kompleksitas kelas dan variasi gaya penulisan karakter.

[Catatan]
Keberhasilan HOG dalam meningkatkan performa klasifikasi dapat dipandang sebagai indikasi tidak langsung adanya ketergantungan spasial antar piksel, mengingat HOG dirancang untuk menangkap struktur gradien lokal dalam wilayah spasial terbatas
"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, accuracy_score

# ambil data dari dataset deeplake
X = ds['images'].numpy()
X = X.reshape(len(X), -1)
y = ds['labels'].numpy().ravel()   # label angka 0–48

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# bikin model KNN baseline
knn = KNeighborsClassifier(n_neighbors=4, n_jobs=-1)

# training
knn.fit(X_train, y_train)

# prediksi
y_pred = knn.predict(X_test)

# evaluasi
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

ds

"""# Setup data"""

import numpy as np
from tqdm.notebook import tqdm

X_raw = []
for img in tqdm(ds['images'], desc="Loading images"):
    arr = img.numpy() if hasattr(img, 'numpy') else np.array(img)
    if arr.ndim > 2:
        arr = arr.squeeze()
    X_raw.append(arr)
X_raw = np.array(X_raw)

y = []
for lbl in ds['labels']:
    val = lbl.numpy() if hasattr(lbl, 'numpy') else np.array(lbl)
    y.append(int(val))
y = np.array(y)

X_raw.shape

X_raw_flat = X_raw.reshape(len(X_raw), -1)
X_raw_flat.shape

"""# Baseline kNN (n_neighbors=4)"""

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

X_train, X_val, y_train, y_val = train_test_split(X_raw_flat, y, test_size=0.2, random_state=42)

knn = KNeighborsClassifier(n_neighbors=4, metric='euclidean')
knn.fit(X_train, y_train)

y_pred = knn.predict(X_val)
acc = accuracy_score(y_val, y_pred)
print(f"KNN validation accuracy: {acc*100:.2f}%")

X_raw.shape

X_train.shape, X_val.shape

"""# Genetic Algo

## Kenapa HOG?

Histogram of Oriented Gradients (HOG) dipilih karena kemampuannya dalam merepresentasikan struktur lokal dan pola tepi yang dominan pada citra tulisan tangan, termasuk karakter kanji pada dataset Kuzushiji-MNIST. Karakteristik HOG yang menghitung distribusi orientasi gradien dalam sel-sel spasial kecil membuatnya relatif robust terhadap variasi gaya penulisan, ketebalan goresan, dan pergeseran lokal yang umum terjadi pada tulisan tangan klasik. Selain itu, HOG tidak bergantung pada informasi intensitas absolut, sehingga lebih stabil terhadap perubahan pencahayaan dan kontras.

Secara empiris, penggunaan HOG menghasilkan peningkatan performa yang signifikan dibandingkan fitur mentah, khususnya ketika dikombinasikan dengan k-NN, yang menunjukkan bahwa informasi gradien lokal lebih diskriminatif dibandingkan nilai piksel individual. Keberhasilan ini secara tidak langsung mengindikasikan pentingnya ketergantungan spasial dan struktur lokal dalam merepresentasikan karakter Kuzushiji, yang tidak dapat ditangkap secara efektif oleh model berbasis fitur mentah atau asumsi independensi fitur.

In simple terms, HOG dipilih karena mampu menangkap struktur tepi dan pola gradien lokal yang menjadi karakteristik utama citra tulisan tangan kanji. Representasi ini lebih robust terhadap variasi gaya penulisan dan deformasi lokal dibandingkan fitur piksel mentah. Peningkatan performa yang diperoleh melalui HOG menunjukkan bahwa informasi lokal dan ketergantungan spasial memainkan peran penting dalam klasifikasi karakter Kuzushiji-MNIST.
"""

import numpy as np
from scipy import ndimage
from skimage.filters import scharr_h, scharr_v
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.random_projection import GaussianRandomProjection
from tqdm.auto import tqdm
from joblib import Memory
import random
import itertools
from sklearn.model_selection import StratifiedKFold

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)

set_seed(42)

# Setup Cache (Data disimpan di folder 'hog_cache_dir')
memory = Memory("./hog_cache_dir", verbose=0)

def compute_gradients(img, kernel_type):
    if kernel_type == "sobel":
        gx = ndimage.sobel(img, axis=1)
        gy = ndimage.sobel(img, axis=0)
    elif kernel_type == "prewitt":
        gx = ndimage.prewitt(img, axis=1)
        gy = ndimage.prewitt(img, axis=0)
    elif kernel_type == "scharr":
        gx = scharr_h(img)
        gy = scharr_v(img)
    else:
        raise ValueError("Unknown kernel")
    return gx, gy

def hog_feature(img, kernel_type, cell_size=4, num_bins=10):
    img = img.astype(np.float32) / 255.0
    gx, gy = compute_gradients(img, kernel_type)

    magnitude = np.sqrt(gx**2 + gy**2)
    orientation = np.mod(np.arctan2(gy, gx), np.pi)

    bin_idx = (orientation / np.pi * num_bins).astype(int)
    bin_idx = np.clip(bin_idx, 0, num_bins - 1)

    H, W = img.shape
    hog = []
    for i in range(0, H, cell_size):
        for j in range(0, W, cell_size):
            hist = np.zeros(num_bins)
            for x in range(i, min(i+cell_size, H)):
                for y in range(j, min(j+cell_size, W)):
                    hist[bin_idx[x, y]] += magnitude[x, y]
            hog.append(hist)

    hog = np.concatenate(hog)
    hog = hog / (np.linalg.norm(hog) + 1e-6)
    return hog

# 2. Fungsi HOG yang di-cache
# Fungsi ini tidak akan menghitung ulang jika input X dan kernel_type sama
@memory.cache
def compute_hog_dataset_cached(X, kernel_type):
    return np.array([hog_feature(img, kernel_type) for img in X])

# --- GENETIC ALGORITHM LOGIC ---

KERNELS = ["sobel", "prewitt", "scharr"]
METRICS = ["cosine", "manhattan", "euclidean"]
WEIGHTS = ["uniform", "distance"]
RP_DIMS = [None, 64, 128]
N_NEIGHBORS_LIST = [3, 4, 5]

def random_individual():
    return {
        "kernel": np.random.choice(KERNELS),
        "metric": np.random.choice(METRICS),
        "weight": np.random.choice(WEIGHTS),
        "rp_dim": np.random.choice(RP_DIMS),
        "n_neighbors": np.random.choice(N_NEIGHBORS_LIST)
    }

def evaluate_individual(ind, X_train_raw, X_test_raw, y_train, y_test): # fitness = akurasi
    # Extract features
    X_train = compute_hog_dataset_cached(X_train_raw, ind["kernel"])
    X_test  = compute_hog_dataset_cached(X_test_raw, ind["kernel"])

    if ind["rp_dim"] is not None:
        rp = GaussianRandomProjection(n_components=ind["rp_dim"], random_state=42)
        X_train = rp.fit_transform(X_train)
        X_test = rp.transform(X_test)

    # KNN with n_neighbors
    knn = KNeighborsClassifier(
        n_neighbors=int(ind["n_neighbors"]),
        metric=str(ind["metric"]),
        weights=str(ind["weight"])
    )
    knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)
    return accuracy_score(y_test, y_pred)

def crossover(p1, p2):
    child = {}
    for k in p1:
        child[k] = p1[k] if np.random.rand() < 0.5 else p2[k]
    return child

def tournament_selection(population, scores, k=2):
    idxs = np.random.choice(len(population), k, replace=False)
    best_idx = idxs[np.argmax([scores[i] for i in idxs])]
    return population[best_idx]


def mutate(ind, p=0.5):
    new_ind = ind.copy()
    if np.random.rand() < p: new_ind["kernel"] = np.random.choice(KERNELS)
    if np.random.rand() < p: new_ind["metric"] = np.random.choice(METRICS)
    if np.random.rand() < p: new_ind["weight"] = np.random.choice(WEIGHTS)
    if np.random.rand() < p: new_ind["rp_dim"] = np.random.choice(RP_DIMS)
    if np.random.rand() < p: new_ind["n_neighbors"] = np.random.choice(N_NEIGHBORS_LIST)
    return new_ind

id_counter = itertools.count()

def run_ga(X_raw, y, population_size=6, generations=5,
           crossover_rate=0.8, mutation_rate=0.5, elite_size=1):

    X_train_raw, X_test_raw, y_train, y_test = train_test_split(
        X_raw, y, test_size=0.2, stratify=y, random_state=42
    )

    population = []

    specific_ind = {
        "kernel": "sobel",
        "metric": "cosine",
        "weight": "distance",
        "rp_dim": None,
        "n_neighbors": 4,
        "_id": next(id_counter)
    }
    population.append(specific_ind)

    while len(population) < population_size:
        ind = random_individual()
        ind["_id"] = next(id_counter)
        population.append(ind)

    best_ind = None
    best_score = -np.inf

    fitness_cache = {}
    feature_cache = {}

    history = []

    for gen in range(generations):
        print(f"\n========== GENERATION {gen+1} ==========")

        scores = []

        for ind in tqdm(population, desc="Evaluating"):
            key = tuple(sorted((k, v) for k, v in ind.items() if k != "_id"))

            if key not in fitness_cache:
                fitness_cache[key] = evaluate_individual(ind, X_train_raw, X_test_raw, y_train, y_test)

            score = fitness_cache[key]
            scores.append(score)

            history.append({
                "gen": gen + 1,
                "id": ind["_id"],
                "fitness": score,
                **{k: v for k, v in ind.items() if k != "_id"}
            })

            if score > best_score:
                best_score = score
                best_ind = ind.copy()

        # --- ELITISM ---
        sorted_idx = np.argsort(scores)[::-1]
        elites = [population[i] for i in sorted_idx[:elite_size]]

        new_population = elites.copy()

        while len(new_population) < population_size:
            p1 = tournament_selection(population, scores)
            p2 = tournament_selection(population, scores)

            if np.random.rand() < crossover_rate:
                child = crossover(p1, p2)
            else:
                child = p1.copy()

            child = mutate(child, p=mutation_rate)
            child["_id"] = next(id_counter)
            new_population.append(child)

        population = new_population

        print(f"Best so far: acc={best_score:.4f} | ID={best_ind['_id']}")

    return best_ind, best_score, history

best_config, best_acc, history = run_ga(X_raw, y, population_size=10, generations=5)

print("\n===== FINAL RESULT =====")
print(best_config)
print("Accuracy:", best_acc)

"""# Save Evolution History"""

import pandas as pd

pd.DataFrame(history).to_json('history.json', indent=4)

df = pd.DataFrame(history)
df.head()

df = pd.read_json('history.json')
df.head()

df.columns

"""# Post-GA Analysis

## Best & Mean Fitness Over Generations
"""

import matplotlib.pyplot as plt
import seaborn as sns

# hitung best dan mean fitness per generasi
stats_per_gen = (
    df.groupby("gen")["fitness"]
      .agg(["max", "mean"])
      .reset_index()
)

plt.figure(figsize=(8, 5))

sns.lineplot(
    data=stats_per_gen,
    x="gen",
    y="max",
    marker="o",
    label="Best Fitness"
)

sns.lineplot(
    data=stats_per_gen,
    x="gen",
    y="mean",
    marker="o",
    label="Mean Fitness"
)

plt.xlabel("Generation")
plt.ylabel("Fitness")
plt.title("Best and Mean Fitness over Generations")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

"""Konfigurasi optimal ditemukan di
generasi 1 dan tidak berubah hingga generasi 5. Ini indikasi
kuat premature convergence. GA terjebak di local optimum
(atau yang dikira optimal) tanpa mengeksplorasi region lain
yang mungkin lebih baik.

## Distribusi kernel per generasi
"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 5))

sns.countplot(
    data=df,
    x="gen",
    hue="kernel"
)

plt.xlabel("Generation")
plt.ylabel("Count")
plt.title("Kernel Distribution per Generation")
plt.legend(title="Kernel")
plt.tight_layout()
plt.show()

"""Sobel muncul 19 kali (38% dari 50
evaluasi), menunjukkan bias seleksi yang kuat. Namun,
dominasi ini justified karena Sobel memiliki mean fitness
tertinggi.
"""

import seaborn as sns

plt.figure(figsize=(12, 7))

ax = sns.violinplot(
    data=df,
    x='metric',
    y='fitness',
    hue='metric',
    inner="quart",
    palette="viridis",
    legend=False
)

plt.title('Distribution of Fitness Across Different Metrics', fontsize=16, fontweight='bold', pad=20)
plt.xlabel('Metric Type', fontsize=12)
plt.ylabel('Fitness Score', fontsize=12)

sns.despine(left=True)
plt.grid(axis='y', linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()

"""## Distribusi fitness per kernel"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8, 5))

sns.boxplot(
    data=df,
    x="kernel",
    y="fitness"
)

plt.xlabel("Kernel")
plt.ylabel("Fitness")
plt.title("Fitness Distribution per Kernel")
plt.tight_layout()
plt.show()

"""- Sobel memiliki mean fitness tertinggi (84.62%)
- Sobel paling consistent (std=4.84%, terendah)
- Prewitt dan Scharr mencapai max fitness hampir sama
dengan Sobel
- Selection pressure favorit Sobel reasonable berdasarkan
mean fitness
"""

mean_fitness_k = (
    df.groupby("n_neighbors")["fitness"]
      .mean()
      .reset_index()
)

plt.figure(figsize=(8, 5))
sns.barplot(
    data=mean_fitness_k,
    x="n_neighbors",
    y="fitness",
    ci=None
)

plt.xlabel("Number of Neighbors (k)")
plt.ylabel("Mean Fitness")
plt.title("Impact of k on Mean Fitness")
plt.tight_layout()
plt.show()

df["rp_dim"] = df["rp_dim"].fillna("None")

rp_analysis = (
    df.groupby("rp_dim")["fitness"]
      .agg(
          Count="count",
          Mean_Fitness="mean",
          Std="std"
      )
      .reset_index()
)

rp_analysis

"""k=4 dan k=5 lebih sering muncul, konsisten dengan best
configuration (k=4)

# Applying Best Config from GA
"""

def hog_feature(
    img,
    cell_size=4,
    num_bins=10,
    eps=1e-6
):
    # normalize pixel
    img = img.astype(np.float32) / 255.0

    # gradients
    gx = ndimage.sobel(img, axis=1)
    gy = ndimage.sobel(img, axis=0)

    magnitude = np.sqrt(gx**2 + gy**2)
    orientation = np.arctan2(gy, gx)  # [-pi, pi]
    orientation = np.mod(orientation, np.pi)  # [0, pi)

    # binning
    bin_idx = (orientation / np.pi * num_bins).astype(int)
    bin_idx = np.clip(bin_idx, 0, num_bins - 1)

    H, W = img.shape
    hog = []

    for i in range(0, H, cell_size):
        for j in range(0, W, cell_size):
            hist = np.zeros(num_bins, dtype=np.float32)

            for x in range(i, min(i + cell_size, H)):
                for y in range(j, min(j + cell_size, W)):
                    b = bin_idx[x, y]
                    hist[b] += magnitude[x, y]

            hog.append(hist)

    hog = np.concatenate(hog)

    # global normalization
    hog = hog / (np.linalg.norm(hog) + eps)

    return hog

def projection_profile(img):
    img = img.astype(np.float32) / 255.0

    horizontal = img.sum(axis=1)  # rows
    vertical = img.sum(axis=0)    # cols

    feature = np.concatenate([horizontal, vertical])
    feature = feature / (np.linalg.norm(feature) + 1e-6)

    return feature

def compute_hog_dataset(X_raw):
    X_hog = []
    for img in tqdm(X_raw):
        hog_feat = hog_feature(img, num_bins=10)
        proj_feat = projection_profile(img)

        combined = np.concatenate([hog_feat, proj_feat])
        X_hog.append(combined)
    return np.array(X_hog)

X_hog = compute_hog_dataset(X_raw)
print(X_hog.shape)  # (N, D)

import pandas as pd
import numpy as np
import time

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB

def train_and_evaluate(model, X, y):
    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=0.2,
        random_state=42,
        stratify=y
    )

    start_time = time.perf_counter()

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    elapsed_time = time.perf_counter() - start_time

    return {
        "acc": accuracy_score(y_test, y_pred),
        "prec": precision_score(y_test, y_pred, average="macro", zero_division=0),
        "recall": recall_score(y_test, y_pred, average="macro", zero_division=0),
        "f1": f1_score(y_test, y_pred, average="macro", zero_division=0),
        "time_sec": elapsed_time
    }

models = {
    "Logistic Regression": LogisticRegression(
        max_iter=1000,
        n_jobs=2,
        multi_class="auto"
    ),
    "Decision Tree": DecisionTreeClassifier(
        random_state=42
    ),
    "kNN": KNeighborsClassifier(
        n_neighbors=4,
        metric='cosine',
        weights='distance'
    ),
    "Naive Bayes": GaussianNB()
}

results = []

for feature_name, X_feat in tqdm([
    ("Raw", X_raw_flat),
    ("HOG", X_hog)
]):
    for model_name, model in models.items():
        metrics = train_and_evaluate(model, X_feat, y)

        results.append({
            "model": f"{model_name} ({feature_name})",
            "use_hog": feature_name == "HOG",
            **metrics
        })

results_df = pd.DataFrame(results)
results_df

"""Berdasarkan hasil eksperimen, k-NN secara konsisten mengungguli model lain baik pada representasi fitur mentah (Raw) maupun HOG. Pada fitur Raw, k-NN mencapai akurasi sebesar 85.60%, jauh di atas Logistic Regression (45.02%), Decision Tree (41.47%), dan Naive Bayes (19.57%). Peningkatan yang lebih signifikan terlihat pada fitur HOG, di mana k-NN mencapai akurasi tertinggi sebesar 90.29% dengan nilai F1-score 0.887, menunjukkan keseimbangan yang baik antara presisi dan recall pada klasifikasi multikelas.

Keunggulan k-NN dalam konteks ini dapat dijelaskan oleh sifatnya sebagai classifier berbasis jarak yang mampu memanfaatkan struktur lokal pada ruang fitur berdimensi tinggi. Representasi HOG menghasilkan vektor fitur yang secara eksplisit mengenkode informasi tepi dan orientasi gradien, sehingga jarak antar sampel menjadi lebih bermakna secara semantik. Hal ini membuat k-NN sangat efektif tanpa memerlukan asumsi distribusi data, berbeda dengan Naive Bayes, maupun pemodelan batas keputusan global seperti Logistic Regression dan Decision Tree.

Selain itu, k-NN menunjukkan robustness terhadap kompleksitas kelas yang tinggi (49 kelas), di mana model parametrik cenderung mengalami underfitting pada fitur Raw atau overfitting pada batas keputusan yang kompleks. Dengan demikian, pemilihan k-NN tidak hanya mengikuti baseline pada penelitian sebelumnya, tetapi juga didukung secara empiris oleh performa yang unggul dan stabil pada kedua jenis representasi fitur.

# Best kNN Classification Report
"""

X_train, X_test, y_train, y_test = train_test_split(X_hog, y, test_size=0.2, stratify=y, random_state=42)

knn = KNeighborsClassifier(
    n_neighbors=4,
    metric='cosine',
    weights='distance'
)

knn.fit(X_train, y_train)

preds = knn.predict(X_test)

from sklearn.metrics import classification_report

print(classification_report(y_test, preds))